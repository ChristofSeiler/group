---
title: "Ã€ la recherche du temps perdu"
description: "Spurious structures in latent space decomposition and low-dimensional embedding methods"
date: "2022-07-28"
categories: "Wisdom"
bibliography: index.bib
---

<!--
TODO: 
- Start with latent space decomposition: 2D, 1D low pass
- Move torus.Rmd into here, also apply cmdscale to same, and UMAP
-->

This post discusses how you can get horseshoe-looking patterns
(sometimes also called cyclic, or U- or V-shaped) in common dimension
reduction methods, from data that are generated by processes that have
little to do with horseshoes, cycles, Us or Vs. 

# Diaconis, Goel and Holmes

In their paper "Horseshoes in multidimensional scaling and local kernel
methods", @Goel2008 looked at situations in which the distance matrix of a
set of objects is concentrated along the diagonal (see @fig-diac1, 
upper left panel). They set out from the example of
the voting records of the members of the 2005 United States House of
Representatives. From these records, they computed all pairwise
distances (or dissimilarities), applied classical multi-dimensional
scaling, and observed a horseshoe pattern.

Generalizing this, and following the ideas of Diaconis, Goel and Holmes,
consider a set of objects whose (dis)similarities are described by the
matrix of pairwise distances $D$. In particular, consider the idealised
situation of matrices like the following one. The elements of $D$ are 0
along the diagonal, and the values increase with the distance from the
diagonal, with some rate $\lambda^{-1}$, until they saturate at some
maximum value, which w.l.o.g. we can set to 1:

<font size=+2> 
$$
D_{ij} = 1 - e^{\displaystyle -\lambda | i-j |},
$$ 
</font> 
where $n$ is a positive integer, $i,j=1,\ldots,n$, and $D$ is
an $n\times n$ matrix.

```{r variousPlotsPar, echo = FALSE}
fwid <- 10.5
fhgt <- 7
```

```{r setup}
set.seed(0xbedada)
library("conflicted")
library("dplyr")
library("ggplot2")
library("gridExtra")
library("reshape2")
library("ggfortify")
library("Rtsne")
library("umap")

plotMatrixAsImage <- function(x) {
  ggplot(melt(x), aes(x = Var1, y = Var2, fill = value)) + geom_tile() + 
    scale_fill_gradient2() +
    ggtitle(deparse(substitute(x))) + coord_fixed() + xlab("") + ylab("") +
    theme(plot.title = element_text(hjust = 0.5))
}

plotMDS <- function(x, tit, flipy) {
  dat <- as_tibble(`colnames<-`(x, c("P1", "P2"))) 
  if (tit %in% flipy)
    dat <- mutate(dat, P2 = -P2)
  ggplot(dat) + 
    geom_point(aes(x = P1, y = P2)) + coord_fixed() + 
    ggtitle(tit) +
    theme(plot.title = element_text(hjust = 0.5)) +
    expand_limits(y = c(-1, 1) * 0.2 * max(abs(dat$P1)))
}

variousMDSPlots <- function(..., perplexity = 30, flipy = character(0)) { 
  grid.arrange(
    plotMatrixAsImage(...), 
    plotMDS(stats::cmdscale(...), "cmdscale", flipy),
    plotMDS(MASS::isoMDS(...)$points, "isoMDS", flipy),
    plotMDS(Rtsne(..., is_distance = TRUE, perplexity = perplexity)$Y, "t-SNE", flipy),
    plotMDS(umap::umap(..., input = "dist")$layout, "UMAP", flipy),
    layout_matrix = rbind(c(1, 2, 3), c(1, 4, 5))
  )
}

diaconisMatrix <- function(lambda, n)
  1 - outer(seq(0, 1, length.out = n), seq(0, 1, length.out = n), 
        FUN = function(a, b) exp(-lambda * abs(a-b)))

D <- diaconisMatrix(lambda = 2, n = 100)

```{r diac1}
#| fig.width: !expr fwid
#| fig.height: !expr fhgt 
#| results: "hide"
#| fig-cap: "The matrix D and various types of multidimensional scaling. Classical (linear) multidimensional scaling, `cmdscale`, and Kruskal's non-metric nultidimensional scaling, `isoMDS`, introduce an apparent curvature, or horseshoe shape, even though the matrix $D$ has a simple band-diagonal structure. Also shown are t-SNE and UMAP."
#| label: fig-diac1
variousMDSPlots(D)
```

For good measure, let us see what happens if we increase `lambda`, i.e.,
shorter range of distance increase away from the diagonal (@fig-diac2).

```{r diac2}
#| fig.width:  !expr fwid
#| fig.height: !expr fhgt 
#| results: "hide"
#| fig-cap: "Like @fig-diac1, but with a different value of  `lambda`, that is, more rapid distance increase away from the diagonal."
#| label: fig-diac2
D2 <- diaconisMatrix(lambda = 6, n = 100)
variousMDSPlots(D2, flipy = c("cmdscale", "isoMDS"))
```

```{r diac3}
#| fig.width:  !expr fwid
#| fig.height: !expr fhgt
#| results: "hide"
#| fig-cap: "Like @fig-diac1 and @fig-diac2, but with even higher `lambda`."
#| label: fig-diac3
D3 <- diaconisMatrix(lambda = 20, n = 100)
variousMDSPlots(D3)
```

The horseshoe becomes even bendier. In some of these plots, the
endpoints (1st and $n$-th object) are laid out closer to each other than
to the objects in the middle, even though this disrespects the true
distances `D2`, `D3`.

Let's look at some of the eigenvectors of the distance matrix `D`, after
double centering. These are used by `cmdscale` (albeit not directly by
the other methods).

```{r Deigenvecpairs}
#| fig.width: 6
#| fig.height: 6
#| fig-cap:  "Eigenvectors (pairs plot)."
#| label: fig-eigenpairs
eigD <- eigen(multivariance:::double.center(D, normalize = FALSE))
colnames(eigD$vectors) <- paste0("EV", seq_len(ncol(D)))
pairs(eigD$vectors[, 1:5])
```

```{r Deigenveclines1} 
#| fig.width: 6
#| fig.height: 4
#| fig-cap: "Eigenvectors, scaled to unit length."
#| label: fig-eigenvecs1
ggplot(melt(eigD$vectors[, 1:5], id = "x") %>% mutate(Var2 = factor(Var2)), 
       aes_string(x = "Var1", y = "value", colour = "Var2", group = "Var2")) +
  geom_line(size = sqrt(2)) + 
  scale_colour_manual("Eigenvectors", values = unname(pals::alphabet(5)))
```

```{r Deigenveclines2}
#| fig.width: 6
#| fig.height: 4
#| fig-cap: "Eigenvectors, scaled by eigenvalue."
#| label: fig-eigenvecs2
ev <- eigD$vectors[, 1:5] * 
  matrix(eigD$values[1:5], byrow = TRUE, ncol = 5, nrow = nrow(eigD$vectors))
ggplot(melt(ev, id = "x") %>% mutate(Var2 = factor(Var2)), 
       aes_string(x = "Var1", y = "value", colour = "Var2", group = "Var2")) +
  geom_line(size = sqrt(2)) + 
  scale_colour_manual("Eigenvectors", values = unname(pals::alphabet(5)))
```

If we think of the (double centered) matrix $D$ as a discretized version
of a differential operator, we see (see @fig-dcd)
that a dominating component is the negative of the second derivative,
i.e., $-d^2/dx^2$, and therefore it is not surprising to see
eigenfunctions that resemble harmonic functions.

$$
\begin{align}
\frac{d^2}{dx^2} f(x) &= \lim_{h\to0} \frac{f(x+h)-2f(x)+f(x-h)}{h^2}\\
-\frac{d^2}{dx^2} f(x) &= k^2 f(x) \Leftrightarrow f(x) \propto e^{ikx} = \cos kx + i \sin kx
\end{align}
$$

```{r scaleDmatrix}
#| fig.width: 4
#| fig.height: 4
#| fig-cap: "Heatmap of doubly centered $D$."
#| label: fig-dcd
plotMatrixAsImage(multivariance:::double.center(D, normalize = FALSE))
```

# Points on a straight line

A straight line in ${\mathbb R}^d$ is parameterized by

$$
 x = at + b  \quad\quad\text{for }t\in{\mathbb R},
$$ where $a$ and $b$ are fixed vectors in ${\mathbb R}^n$ (slope and
offset). In the code below, we also add some Gaussian noise, with
$\sigma=0.5$.

```{r straightline1}
#| fig.width:  !expr fwid
#| fig.height: !expr fhgt
#| results: "hide"
#| fig-cap: "Application of dimension reduction methods to a straight line in feature space. The straight line structure is reasonably preserved, although with some peculiarities in the case ot $t$-SNE."
#| label: fig-straightline1
d <- 100   # dimension of the space
n <- 42    # number of points 
t <- seq(0, n, by = 1) %>% matrix(nrow = 1)          # row vector
a <- runif(d) %>% matrix(nrow = d, ncol = 1)         # column vector
b <- runif(d) %>% matrix(nrow = d, ncol = length(t)) # recycle
x <- a %*% t + b
x <- x + matrix(rnorm(prod(dim(x)), sd = 0.5), ncol = ncol(x), nrow = nrow(x))
dm <- as.matrix(dist(t(x)))
variousMDSPlots(dm, perplexity = 10)
```

## With saturation

Instead of the Euclidean distances, add some saturation (underestimate
long distances). For instance, we can take the function
$s(x) = 1-e^{-x/x_0}$, for which $s(0)=0$ and $\lim_{x\to\infty}s(x)=1$.

```{r saturationfunction}
#| fig.width: 3.5
#| fig.height: 2
#| results: "hide"
#| fig-cap: "Saturation function"
#| label: fig-saturationfunction
sat <- function(x, x0 = median(x)) { x0 * (1 - exp(-x/x0)) }
ggplot(tibble(x = as.vector(dm),
              y = as.vector(sat(dm))), aes(x = x, y = y)) + 
  geom_line() + xlab("d") + ylab("sat(d)")
```

```{r straightline2}
#| fig.width:  !expr fwid
#| fig.height: !expr fhgt
#| results: "hide"
#| fig-cap: "Like @fig-straightline1, but with some saturation added (underestimation of long distances)."
#| label: fig-straightline2
variousMDSPlots(sat(dm), perplexity = 11)
```

## With noise

Back to the Euclidean distances, but with noise on the points' coordinates.

```{r straightline3}
#| fig.width:  !expr fwid
#| fig.height: !expr fhgt
#| results: "hide"
#| fig-cap: "Like @fig-straightline1, but with noise."
#| label: fig-straightline3
dmnoisy <- as.matrix(dist(t(x + matrix(rnorm(prod(dim(x)), sd = 5), ncol = ncol(x), nrow = nrow(x)))))
variousMDSPlots(dmnoisy, perplexity = 11)
```

# Some further curiosities

## 2D low pass

In the code below, we create a data matrix of independent, identically
distributed random numbers (`white.noise`), to which we apply a 2D
smooothing operator (`filter2` from the `EBImage` package) with
bandwidth `b`. The resulting matrix `x` has size `n` times `k`. (The
matrix `white.noise` has an additional `b` rows and columns padded to
the left, right, top and bottom to avoid having to make choices about
how `filter2` should deal with the boundaries.)

```{r 2dlowpasssetup}
n <- 100
k <- 150    
b <-  35
white.noise <- matrix(rnorm((k+2*b) * (n+2*b)), nrow = n+2*b, ncol = k+2*b)
smoothened  <- EBImage::filter2(white.noise, EBImage::makeBrush(b))
x <- smoothened[ b+(1:n), b+(1:k) ] 
```

Let's do a PCA plot of `x`, using the first two principal components

```{r 2dlowpassprcomp}
#| fig.width: 4 
#| fig.height: 3.5
#| results: "hide"
#| fig-cap: "PCA plot of `x`, using the first two principal components."
#| label: fig-2dlowpassprcomp
autoplot(prcomp(x)) + coord_fixed()
```

We see an (almost-closed) cycle. If the data were from a single-cell
RNA-seq experiment (with, say, `n` cells and `k` genes), it could be
tempting to see a cellular trajectory that we might think is related to
a differentiation process and/or the cell cycle. There are software
packages that help fit curves into such plots, which could be
interpreted as "pseudo-time". In many cases, this will be very well (and
useful and true) - but in this case we know that the data-generating
process had nothing to do with time or some other underlying
one-dimensional process or phenomenon. What is going on?

Below, we plot the matrix in false color representation, as well as its
covariance matrix.

```{r 2dlowpassmatrices}
#| fig.width: 9
#| fig.height: 3.5 
#| results: "hide"
#| fig-cap: "Matrix `x` in false color representation."
#| label: fig-2dlowpassmatrices
grid.arrange(plotMatrixAsImage(x), plotMatrixAsImage(cov(x)), ncol = 2)
```

```{r 2dlowpassmds}
#| fig.width:  !expr fwid
#| fig.height: !expr fhgt
#| results: "hide"
#| fig-cap: "`dist(x)`."
#| label: fig-2dlowpassmds
variousMDSPlots(as.matrix(dist(x)))
```

## 1D low pass

We can be even more minimalistic with the non-randomness of the input
data. In the code below, the `k` times `n` matrix `rw` is filled with
the numbers from a random walk (a realisation of 1D Brownian motion).

```{r 1dlowpassmatrices}
#| fig.width: 8
#| fig.height: 4 
#| warning: FALSE
#| results: "hide"
#| fig-cap: "Random walk `rw`."
#| label: fig-1dlowpassmatrices
rw <- matrix(cumsum(rnorm(k * n)), nrow = k, ncol = n)
grid.arrange(plotMatrixAsImage(rw), autoplot(prcomp(rw)) + coord_fixed(), 
             ncol = 2)
```

We again see the horseshoe. And interesting patterns in the first five
principal components:

```{r 1dlowpasspairs}
#| fig.width: 6
#| fig.height: 6 
#| results: "hide"
#| fig-cap: "First five principal components of `rw`."
#| label: fig-1dlowpasspairs
pairs(prcomp(rw)$x[, 1:5])
```

```{r 1dlowpassmds}
#| fig.width:  !expr fwid
#| fig.height: !expr fhgt
#| results: "hide"
#| message: FALSE
#| warning: FALSE
#| fig-cap: "MDS plots of `rw`."
#| label: fig-1dlowpassmds
variousMDSPlots(as.matrix(dist(rw)))
```

Now, instead of a Brownian motion, let's just take a series of
independent random numbers and subject them to a low-pass filter of
window size `h` (i.e., a running-average smoother).

```{r runavgmatrices}
#| fig.width: 8
#| fig.height: 4 
#| warning: FALSE
#| results: "hide"
#| fig-cap: "Just a series of independent random numbers subjected to a low-pass filter of window size `h` (i.e., a running-average smoother)."
#| label: fig-runavgmatrices
h  <- 20
rw <- matrix(
    stats::filter(rnorm(k*n + h), rep(1, h))[ h/2 + 1:(n*k) ],
    nrow = k, ncol = n) 
grid.arrange(plotMatrixAsImage(rw), autoplot(prcomp(rw)) + coord_fixed(), 
             ncol = 2)
```

```{r runavgmds}
#| fig.width:  !expr fwid
#| fig.height: !expr fhgt
#| results: "hide"
#| message: FALSE
#| warning: FALSE
#| fig-cap: "MDS plots of `rw`."
#| label: fig-runavgmds
variousMDSPlots(as.matrix(dist(rw)))
```

# Conclusion

Embeddings of high-dimensional data into lower dimensional spaces are
useful; but they can also create apparent one-dimensional ("time-like")
patterns that have little to do with the data-generating process. Be
aware.

# Session info

```{r sessionInfo}
devtools::session_info()
```
